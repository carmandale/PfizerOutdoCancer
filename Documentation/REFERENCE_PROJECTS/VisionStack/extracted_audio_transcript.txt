 Hi and welcome to another very exciting tutorial. In this video we're going to explore some amazing possibilities with Apple Vision Pro, namely hand tracking, scenery construction and interactions with the real world. We're going to build on top of some Apple sample code and build this app that lets you place cubes using a position of your left fingertip as a pointer and you right hand as a trigger. This is possible by using ARKID in a Vision OS immersive space. Be sure to subscribe to this channel to not miss any future tutorials about Apple Vision Pro and let me know what you're most interested regarding all the new possibilities. And now let me introduce you again to our application that we're going to build. You've already seen what is possible but just let's dissect this here a little bit so we have a windowed application that we have here on the left. We have our instructions. We have this 3D model that we can play around with here a little bit and we can start our stacking procedure if we'd like and also stop it so we have a toggle button here. So this is the UI that is already in place and this is also not an introduction into user interface design for Vision Pro. So let me know if you're interested in that in the comments below. But we're going to have a look at the project here now for a second so that you see what's already there and what we're going to build on top. So this is the project and as you can see we have a little helper group here. We have the content view. We have the normal entry point for every SwiftUI application and this is also where we are going to add our immersive view later since the possibility to make the hand tracking available using ARKit and also scene understanding is something that you only get in an immersive view. So this is where we're going to add that later. Then in content view we're defining our user interface which is pretty standard SwiftUI stuff here. We have our toggle that we're going to use later to open our immersive space using these two environments or the open immersive space environment and the dismiss immersive space so that we can actually pass along the immersive space that we'd like to load. Also quite important is to add two keys and values for the or in the property list which is NSHans tracking usage description and world sensing usage description. These are the two features that we're using and we need to inform our users about that here in our P list. And now as for the packages that are also available here you can see we have our reality kit content here and this is where Xcode and reality composer pro are managing our assets or our 3DS. It's in a very efficient way so this is where our cube for the user interface lies. So here you just created a usda file with a little scene that I called cube and the immersive scene here. This is just what comes by standard or by default when you create a project. So we have our cube here, nothing fancy and we're going to load that in a helper function here which I call sample model and here we just use model 3D. We can pass along a parameter or the orientation property to this sample model so that we can rotate it a little bit and make it look cooler using the rotation 3D effect modifier. And then what we also have is a drag modifier which is derived from some apple sample code to actually make the rotation effect that we have here possible so dragging around. This requires this modifier that we add to our 3D model here in our content view. So as you can see we have the drag rotation modifier here and we give it its limits for yaw and pitch. So that's actually it. One more extension that we're going to have a look at later which is just creating a entity with the circle that you've already seen that we're going to track on our fingertips. So there's also nothing fancy really and we're going to work with that in a second. So this is all the code there is now so let's get started creating our stacking view that we're going to use inside our immersive view so I'm going to click on my content view press command and on my keyboard to create a new SwiftUI file and I'm going to call that stacking view. So this is just a normal SwiftUI view that we're working with here now and we're going to import reality kit since we're going to use that and we're also going to import reality kit content if we need something from the package that we have here. This is the reality kit content that we're importing right there. So in our struct just keeping the body and replacing or moving our text here and add a reality view. Reality view gets content or has already a content entity that we can work with and add some pseudo code first and then work our way up from there. So once our reality view gets created we want to add our content entity that we're going to set up later and this content entity is going to hold all the cubes and also our fingertips that we're going to track to our fingers. And once we did that we're going to specify some asynchronous tasks. So the first task is going to be to actually run our AR kit session. So this session is going to be required to actually track our hands and also track the environment and add our mesh and so on. Then the second task is going to be to process our hand updates so that we know where our hands are and what we're going to do with them. And the third task is going to be to process our world reconstruction that we're getting using a AR kit. And then the last thing that we're going to do with our reality view is going to use it to actually wait for a pinch gesture that we perform with our right hand. So we're adding a gesture here and since we're in a reality view we can use spatial tab gesture and from this tab gesture we're using target to any entity. So we're not looking for a specific entity. We're looking for everything that we get and we're waiting for the on-ended stage of this gesture. And then we can just add a closure here and here we're receiving some value and what we can then do is also asynchronously place our cube. So this is some real code, some pseudocode that we're going to work with and we have to place this underscore with an in of course. But this is the pseudocode that we're going to work with but now we have to talk about how we're going to manage all of that efficiently. And especially when working together with a AR kit it is a good practice to use the MVVM pattern for example which stands for model view model. This is a software architectural pattern that separates the user interface, the view from the business logic and the data which we're calling model and using an intermediary view model and the view model handles the logic for the UI acting as a bridge that kind of decouples the model from the view and making your code more modular easier to test and also easier to maintain. And for that we're going to create a new class which I'm going to use a Swift file for and we're going to call that hand tracking view model. And here we're also going to use a reality kit. We're also going to use Swift UI. We're going to use a R kit which is going to help us with hand tracking and scene understanding and again if we're going to need it reality kit content here. Since we're performing things regarding the user interface we also define this using the main actor singleton for defining our class. We're going to call it hand tracking view model making this an observable object. And we're going to see why this is useful in a second. So if we're going back to stacking view then here we can define a state object called this model and then initialize it with our hand tracking view model and in Swift UI state object is a property wrapper that is used to create and manage. Now a reference to an observable object that we just created with our class that is our source of truth for the view. So this observable object typically is a class that conforms now to the observable object protocol and you can have published properties that when changed signal Swift UI to update the views that depend on this data. And this is what we're going to use in our context of MVVM to actually make our changes visible and to detach our user interface code from the main business logic. So with that in place we can now deal with a lot of interesting things. So first of all in this class we're defining a private session which we only have here which is going to be an AR kit session. We're going to use that for scene understanding and for hand tracking but also for hand tracking. We're going to use a hand tracking provider so there's going to be called hand tracking and we're going to initialize it with a hand tracking provider. Also what we're going to need is a scene reconstruction provider. So we're going to call it scene reconstruction and this is going to be a scene reconstruction provider and this is going to be used to actually understand if we are seeing a table, ground floor, you could also detect special kinds of furniture, sofa, TV, doors, etc. So it's very handy. Then we're going to have a variable which is going to be called content entity which is going to receive all of our entities that we're going to place like cubes, the fingertips and so on. Then we're also going to have a variable called mesh entities which is going to receive a dictionary holding UUID keys and model entities as values. This is where we're going to store the meshes that we create for the scene reconstruction. So let you just quickly, correctly, initialize that here. Now we're going to move on to our fingers. So let's create some finger entities which are of type also again a dictionary with a key of hand anchor and its corality and a model entity as a value. And we're going to initialize that right away with our fingers. So we forward the left hand side, we're going to create a fingertip and the same thing for the right hand side, create finger tip. And now as far as the last property that we're going to need in our class, we're going to create a private variable last cube placement time which is a time interval that we're going to use to not add too many cubes at the same time or a lot to figure pinch too many times to make this a little bit more efficient. And now we can go on with the methods, our functions. So let's get started with setup, content entity, which is going to return an entity. And what we're going to do first of all is adding our fingertips to our content entity that we have already created here and initialized here, but we're going to need our fingertips in any case. So we're going to iterate through finger entities and its values. And for each entity, we're going to use our content entity and just add a child, which are the two fingertips. And then we would simply return our content entity. And then we can actually go back to our stacking view. And here we can now add our content entity to our reality view. So we use the content that we get in our closure here. So now we're using our model for the first time and just call setup content entity from here. Going back, we're now going to deal with our AR session. So we're going to a arcade session. We're going to create a function call at run session, which is going to perform asynchronous codes or we're defining it like that. And in here, we're going to use a do try catch statement. And what we're going to try is we want to await for our session that we created to run. And we're going to provide it with some data providers, which for one instance, we want scene reconstruction. So we're passing along scene reconstruction. And we also want hand tracking. So we're also going to pass that along. And if that fails, we're going to use a catch statement to just print failed to start session. And we can use string interpolation to just print out the exact error if you want to debug this. And now that we have that, could go back to our stacking view. Since it's working asynchronous, we use a weight here as well. Use model and run our session. Since it's in a task block, we can use this construct, await, and then this happens nicely asynchronously. And seems that we've got one error here that we need to handle. And we forgot some parentheses here. So just adding them to our function. So now that we imagine that we actually are running our session, we perform hand tracking, we perform scene reconstruction. Let's first of all, maybe tackily hand tracking. Therefore, we're going to use process hand updates as a function here, also working asynchronously. And here, what we get, and here, what we do is we're using our hand tracking provider. And we iterate through its updates. So we're using a for loop here. And we await that we receive updates. And we're getting them from our hand tracking provider and its anchor updates. So for each update that we get, we're going to perform the following code. So first of all, we need to get the hang the hand anchor from our update. So we use update dot anchor. This is our hand anchor now. So we're using a guard statement, taking our hand anchor. And we ask if it is tracked and if it is tracked, everything is fine. If not, we will simply use continue here to skip this iteration of the loop. And wait for the next one. And hopefully we'll get a, our hand is tracked then. And now in the next line, we can now assume that we are actually having a hand anchor and that it is tracked. So we can actually define a fingertip. And using hand anchor and using the hand skeleton, we can now ask for a specific joint. And we're going to for the index finger tip. Now, we'll also need to check if this is tracked. So we're using a guard statement again. And we're using the fingertip. We're asking if it is tracked. And if this is not equal to nil. So we have something we can continue with our code. So else we also have to continue. But this time, meaning we want to skip again this iteration. So once we reach this next line of code, we know the fingertip is actually tracked. And if we have the fingertip, then we can get the transform from the wrist to the origin of our current scene. So we're going to ask for the origin from wrist by using the hand anchor. And then just call origin from anchor transform, which is the location and orientation of our hand in world space. So having that, we can continue using the wrist or getting the wrist from index. So we're using the fingertip and then the anchor from joint transform, which is, as you can see, the position and orientation of the joint relative to the base joint of the skeleton. So this is the wrist from index. And now since we're working with transforms and these are matrices, what we can do to get the transform from origin to the index fingertip, what we want to do is defining origin from index. We can use a matrix multiplication and use origin from wrist times wrist from index. And we need to unwrap this here for this to work. And this origin from index is now what we're going to use to actually position our finger entities that we created. So we're using the finger entities. We're using our key hand anchor and its chirality to actually access the correct fingertip. And then we set the transform matrix, or we position it, to the origin from index that we just calculated and we can relative to know. So we do not have to make this relative to another entity. So we pass along nil. But let's maybe quickly hold on because when we try to see our hand updates, now our fingertips, we also need to display our immersive scene. So let's make some modifications for that first, because otherwise we wouldn't see anything. So let's go back to our vision stack abstract here. Let's define our immersive space. Give it an ID, which I'm going to call stacking space. And let's add our stacking view right here. And now let's go into our content view, which we will need to actually toggle or activate our immersive space. So we're going to open it and to do so, we should define a task. Since this is going to be a synchronous task and we can check if is showing is true. And if that is the case, then we can use weight and open our immersive space, passing along an ID. And open immersive space is the environment variable that we've defined here right on top. So we pass along our stacking space and else we will dismiss. Also, wait, dismiss our immersive space, having a nice little toggle function. And if you're wondering where this is showing is coming from, this is the new value of our unchanged block here. So we have an old value, which we're going to skip. We're using that. And then we get a new value. And this is in our case, a bullet we can listen for. And now the last thing that we need to do before we can test our app is to go back into our stacking view and go to our next pseudocode statement, which is process our hands, updates. And here again, we will use a weight, use our model and simply call process hand updates. And then we can run this on our vision pro and see what we get. And indeed, here we are. We're looking in our hands, moving them around. And as you can see, it tracks them, all two dots nicely, two hour two finger tips. So we can now move on to our next task on our checklist, which is actually the scene reconstruction. So let's go back in our hand tracking view model and tackle this problem right at a bottom of our file. And scene reconstruction actually works from the basic concept very similarly to what we did before. So we're going to process reconstruction updates, reconstruction updates, and we're going to also make this function asynchronously usable. And again, we're using a for loop, and we await our update, which we get from scene reconstruction and its anchor updates. And now for our scene reconstruction, we're going to get a lot of mesh that we can store and hold purpose of our scene reconstruction is to actually lay out our floor and the objects that are around us and give them a collision component so that our cubes can actually fall onto them and react to the physical environment. So what we need for that is basically a shape that we can create and use as the shape for our collision components. So we're going to create such a shape by trying to using a weight statement and try a weight in combination to use a shape resource and then generate static mesh from and what we're using here for is our update and our anchor that we get from or in every update. And if this does not work, then we will simply continue and skip this iteration just as we did before with our hand updates. And now we have in this scene reconstruction update, we can now switch through different cases of what happens. So we switching through different events and the first event that we're going to have a look at is the event added. And in that case, what we need to do is create a first entity which is going to be a model entity. And we're going to set the transform of this entity by using or creating a transform object with a matrix which we're getting from our update and its anchor and its origin from anchor transform. And with that, we have the position of our entity. Then that's very important for this work is to actually set its collision component by creating a collision component. And we initialize it with some shapes that we created and pass them along as an array. And we're going to use our shape that we just created for that. And we're going to set this to static. So this is the first component that we need. A second component is an actual physics body. Let me just make some space here so that you can see and everything. So we're using our entity using and giving it a physics body by just initializing a physics body component which makes it possible to add physics. And for and to make this interactable, what we're also going to do is use our entity components and use the set function to set an input target component and initialize that right here. And now to update our mesh when performing more scene reconstruction, we're going to use our mesh entities dictionary that we created. And as a key, we use our update anchor and get its ID which is also a UID which we have set as a key for mesh entities. And the value is the entity itself. And then we can use our content entity and add our mesh entity or the entity that we just created as a child. And now if we're going to the other case, if we got an update, so we're using the updated event here, we're going to create a guard let statement to check if we can get an entity from our mesh entities by accessing again our update anchor and its ID. And if not, we're going to just throw a fatal error with some message that you're welcome to figure out. And then we again set or update the transform you initializing that with a transform and matrix using our update anchor and its origin from anchor transform. Again, collision should be available. We're updating the shapes as well, passing long shape. And in the case that it got removed by scene reconstruction, just moving along the dot here a little bit. So in case it removed, got removed, we're using the mesh entities dictionary accessing it with our update anchor and its ID. And then we remove that from parent and also, since we're removing that from our entity list, we're also going to remove it from our dictionary for the key update anchor and ID. And with that, we have exhausted the switch statement and we have scene reconstruction now, which is cool. And this brings us to the less step of our to do list, which is actually be able to place cubes. So we're writing a function, a function called place cube. And also, it's going to be able to be called asynchronously. And since we want to place it where the left fingertip is so that we can kind of point at where we want to spawn our cube, we're going to get our left finger position by using our finger entities, accessing the left one, then get its transform and its translation, which is the position of that case. And if that's not possible, we return. Since we cannot continue then, then we have to actually make a little adjustment so that is positioned nicely. So we're going to create a new placement location, which is going to be the left finger position. And we're just going to add a little something to that matrix using an SMID object of type float in that case. And we're just going to move our cube above 5 centimeters below our fingertip and we make no further adjustments here. And then since we do not have a cube yet, we're going to create one. So let's create an entity here. It's going to be a model entity with a mesh. We can simply use a generate box with a specific size. Let's say point one right here. Then we will add material or a list of materials, which in our case is just going to be a simple material of a specific color. Let's say it's going to be system blue and we're not going to make it metallic, which completes our material. Now let's also give it a collision shape, which should be pretty simple here since we have a box again. So we generate a box. And as you can see, we can simply pass along a matrix here. Let's say with a repeating value of point one. And then we can give our cube a mass of a balled one. Point zero with that we have an entity. We have our box, but we still need to position it. So we're accessing it. We said it's position. And now we can simply pass along our placement location. And it's not going to be relative to any other entity. Now as with our mesh components, we also need to get this entity, some components, the input target component. So we use entity, its components, and set it to input target component. We initialize that. And we will set it to be allowed with input types of indirect, you have a direct and indirect input types. We're going to allow indirect input types right here. Then we can give it yet another component. Let's use the set function here as well, which is going to be a grounding shadow component. And it should cast shadow. So we set this to choose so that it actually casts shadows on surfaces like a table or the floor. And then also we need to give it some physical material or physics material to behave in a certain way. So let's also define a material here, which is a physics material resource. We can simply call generate generate here. Let's set the friction to point eight. And the restitution to zero. And last but not least, we will also give it its physics body components. So again, using the entity, its components, we set the physics body component with a shape, mass, material, and so on. So for the shapes, we pass along entity, collision, shapes that we already defined. For the mass, 1.0 material, we have our material. And our mode is going to be dynamic so that forces and collisions can control the movement. And now all that's left to do is use our content entity as a container and add our entity as a child here. We're going back to our stacking view. We have our pseudocode here. So one last task here is to actually process the world reconstruction. So again, wait, use the model and we use process reconstruction updates. And in our gesture recognizer, we again use our model and we want to place our cube. And now let's run it and see if it works. And indeed, we can place objects, we can interact with our objects. And I think this is so cool placing elements in mixed reality and interacting with them. And as you can see, see reconstruction and hand tracking is quite simple. If you know how everything works together, and I hope you will have some fun and come up with amazing ideas on how to use that. Let me know what do you find most interesting about VisionOS, VisionPro and what you're excited about. Be sure to subscribe to this channel to not miss any future tutorials. I thank you so much for watching and I'll see you in the next one. Bye.
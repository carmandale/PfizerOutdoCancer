Displaying an entity that follows a person’s view
Create an entity that tracks and follows head movement in an immersive scene.
Download
visionOS 2.0+
Xcode 16.0+
Overview
This sample uses world-tracking data from ARKit in visionOS to create and display a 3D entity that dynamically moves in front of a person’s view. As the following video shows, the floating sphere’s position updates based on the person’s head movement, to ensure the object stays visible and smoothly follows their view:

Play
Extend the floats to enable calculations
The sample adds functionality to existing class types by extending SIMD3<Float>, SIMD4<Float>, and simd_float4x4:
import Foundation
import simd
import RealityKit


/// The type alias to create a new name for `SIMD3<Float>`.
typealias Float3 = SIMD3<Float>


/// The type alias to create a new name for `SIMD4<Float>`.
typealias Float4 = SIMD4<Float>


/// The type alias to create a new name for `simd_float4x4`.
typealias Float4x4 = simd_float4x4
To include these data types in the extension to their associated class, the sample associates each of the entry points using a typealias.
The Float3 extension includes the following methods:
init(_:), to create a Float3 from a Float4
length(), to calculate the total length of the Float3
normalized(), to calculate the normalized vector of the Float3
import Foundation
import simd
import RealityKit


typealias Float3 = SIMD3<Float>


// ...


extension Float3 {
    /// The initializer of a `Float3` from a `Float4`.
    init(_ float4: Float4) {
        self.init()
        
        x = float4.x
        y = float4.y
        z = float4.z
    }
    
    // Calculate the total length by taking the square root of the product of the provided float.
    func length() -> Float {
        sqrt(x * x + y * y + z * z)
    }
    
    // Calculate the normalized vector of the float.
    func normalized() -> Float3 {
        self * 1 / length()
    }
}
The Float4 extension contains the toFloat3() method that converts a Float4 value to Float3:
import Foundation
import simd
import RealityKit


typealias Float4 = SIMD4<Float>


// ...


extension Float4 {
    // Ignore the W value to convert a `Float4` into a `Float3`.
    func toFloat3() -> Float3 {
        Float3(self)
    }
}
The Float4x4 extension includes the following methods:
translation(), to get the transform information in the form of a Float3
forward(), to get the forward-facing vector
import Foundation
import simd
import RealityKit


typealias Float4x4 = simd_float4x4


// ...


extension Float4x4 {
    // Identify the translation value from the `float4x4` and convert to a `Float3`.
    func translation() -> Float3 {
        columns.3.toFloat3()
    }
    
    // Identify the forward-facing vector and return a `Float3`.
    func forward() -> Float3 {
        columns.2.toFloat3().normalized()
    }
}
Update the entities over time
The sample sets up a custom system and component to handle updates in real time:
import SwiftUI
import RealityKit


struct ClosureComponent: Component {
    /// The closure that takes the time interval since the last update.
    let closure: (TimeInterval) -> Void


    init (closure: @escaping (TimeInterval) -> Void) {
        self.closure = closure
        ClosureSystem.registerSystem()
    }
}
The component contains the closure variable to track the time. On initialization, it registers ClosureSystem into the reality view.
The ClosureSystem constructs a query using the EntityQuery to retrieve all entities with the ClosureComponent from the scene. Then it passes the delta time, which is the elapsed time since the last update, to the closure variable for each entity:
import SwiftUI
import RealityKit


struct ClosureSystem: System {
    /// The query to check if the entity has the `ClosureComponent`.
    static let query = EntityQuery(where: .has(ClosureComponent.self))
    
    init(scene: RealityKit.Scene) {}
    
    /// Update entities with `ClosureComponent` at each render frame.
    func update(context: SceneUpdateContext) {
        for entity in context.entities(matching: Self.query, updatingSystemWhen: .rendering) {
            guard let comp = entity.components[ClosureComponent.self] else { continue }
            comp.closure(context.deltaTime)
        }
    }
}
Implement head tracking
visionOS supports WorldTrackingProvider from ARKit to get live data about a device’s position. World tracking requires an ARKitSession and a device that supports world tracking. The sample uses the HeadPositionTracker to initialize the ARKit session and the WorldTrackingProvider:
import SwiftUI
import RealityKit
import ARKit


class HeadPositionTracker: ObservableObject {
    /// The instance of the `ARKitSession` for world tracking.
    let arSession = ARKitSession()


    /// The instance of a new `WorldTrackingProvider` for world tracking.
    let worldTracking = WorldTrackingProvider()


    init() {
        Task {
            // Check whether the device supports world tracking.
            guard WorldTrackingProvider.isSupported else {
                print("WorldTrackingProvider is not supported on this device")
                return
            }
            do {
                // Attempt to start an ARKit session with the world-tracking provider.
                try await arSession.run([worldTracking])
            } catch let error as ARKitSession.Error {
                // Handle any potential ARKit session errors.
                print("Encountered an error while running providers: \(error.localizedDescription)")
            } catch let error {
                // Handle any unexpected errors.    
                print("Encountered an unexpected error: \(error.localizedDescription)")
            }
        }
    }
The HeadPositionTracker contains the originFromDeviceTransform() method to get the devices’s transform in real time:
func originFromDeviceTransform() -> simd_float4x4? {
    /// The anchor of the device at the current time.
    guard let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: CACurrentMediaTime()) else {
        return nil
    }


    // Return the device's transform.
    return deviceAnchor.originFromAnchorTransform
}
Display the sphere that follows the view
Device tracking is accessible within immersive spaces. The sample creates a custom view that uses a reality view to place a 3D sphere in front of the device’s forward direction at a set distance.
Note
Device-tracking data isn’t available in visionOS apps that only display a SwiftUI window view or a SwiftUI volumetric view.
import SwiftUI
import RealityKit


struct HeadPositionView: View {
    /// The tracker that contains the logic to handle real-time transformations from the device.
    @StateObject var headTracker = HeadPositionTracker()


    var body: some View {
        RealityView(make: { content in
            /// The entity representation of the world origin.
            let root = Entity()


            /// The size of the floating sphere.
            let radius: Float = 0.02


            /// The material for the floating sphere.
            let material = SimpleMaterial(color: .cyan, isMetallic: false)


            /// The sphere mesh entity.
            let floatingSphere = ModelEntity(
                mesh: .generateSphere(radius: radius),
                materials: [material]
            )


            // Add the floating sphere to the root.
            root.addChild(floatingSphere)


            // ...
        }
    }
}
The view creates two entities: the root and the floatingSphere.
The view sets the ClosureComponent to the root, creates the currentTransform property to determine the headset’s current location, and calculates a smooth target position for the floating sphere in front of the device:
var body: some View {
    RealityView(make: { content in
        // ...
    
        /// The distance that the content extends out from the device.
        let distance: Float = 1.0


        root.components.set(ClosureComponent(closure: { deltaTime in
            /// The current position of the device.
            guard let currentTransform = headTracker.originFromDeviceTransform() else {
                return
            }


            /// The target position in front of the device.
            let targetPosition = currentTransform.translation() - distance * currentTransform.forward()


            /// The interpolation ratio for smooth movement.
            let ratio = Float(pow(0.96, deltaTime / (16 * 1E-3)))


            /// The new position of the floating sphere.
            let newPosition = ratio * floatingSphere.position(relativeTo: nil) + (1 - ratio) * targetPosition


            // Update the position of the floating sphere.
            floatingSphere.setPosition(newPosition, relativeTo: nil)
        }))


        // Add the root entity to the `RealityView`.
        content.add(root)
    }, update: { _ in })
}
The setPosition() method moves the sphere to the new position over a set rate of time, applying a smoothing effect to the sphere.

Placing entities using head and device transform
Query and react to changes in the position and rotation of Apple Vision Pro.
Download
visionOS 2.0+
Xcode 16.0+
Overview
This sample code project demonstrates how to create and display content that appears at a person’s head location, and follows a person’s view as they move their head in immersive spaces. It uses AnchorEntity and queryDeviceAnchor(atTimestamp:) to get the transform of the person’s head and Apple Vision Pro to place content relative to them.
This sample creates the following two views and allows you to toggle between them:
A hummingbird and a feeder directly in front of the person wearing the device.
A hummingbird that flies to stay in the view of the person wearing the device.

Play
The sample code project uses RealityKit and ARKit, respectively, to position the entities relative to the person. You can run the sample app in either Simulator or on-device.
Note
See Design considerations for vision and motion and Motion in the Human Interface Guidelines for guidance on continuously head-tracked entities.
Show entities at a person’s head position
To launch the hummingbird feeder at the position of the wearer’s head, the sample uses AnchorEntity with the anchoring target of AnchoringComponent.Target.head. This target provides the center of the wearer’s head, rather than the position of the device itself. You can only use AnchorEntity in an immersive space. Although it allows you to anchor content to the wearer’s head, you can’t access its transform because there’s no authorization required. If you attempt to access the transform, the property returns the identity transform instead.
Note
You can get the transform of an AnchorEntity with a different AnchoringComponent.Target, such as a hand, by using a SpatialTrackingSession and requesting authorization from the person using the app.
The sample creates an AnchorEntity that anchors to the wearer’s head, and sets the AnchoringComponent.TrackingMode to once to stop tracking after the initial anchor. The head-positioned entity root contains both the feeder entity and the hummingbird entity, which the sample loads from Reality Composer Pro. The app adds the root entity as a subentity of the head anchor to track it. The sample then offsets the feeder from the center of the wearer’s head by setting the position.
func startHeadPositionMode(content: RealityViewContent) {
    // Reset the rotation so it aligns with the feeder.
    hummingbird.transform.rotation = simd_quatf()
    
    // Create an anchor for the head and set the tracking mode to `.once`.
    let headAnchor = AnchorEntity(.head)
    headAnchor.anchoring.trackingMode = .once
    headAnchor.name = "headAnchor"
    // Add the `AnchorEntity` to the scene.
    headAnchorRoot.addChild(headAnchor)
    
    // Add the feeder as a subentity of the root containing the head-positioned entities.
    headPositionedEntitiesRoot.addChild(feeder)
    
    // Add the hummingbird to the root containing the head-positioned entities and set the position to be further away than the feeder.
    headPositionedEntitiesRoot.addChild(hummingbird)
    hummingbird.setPosition([0, 0, -0.15], relativeTo: headPositionedEntitiesRoot)
    
    // Add the head-positioned entities to the anchor, and set the position to be in front of the wearer.
    headAnchor.addChild(headPositionedEntitiesRoot)
    headPositionedEntitiesRoot.setPosition([0, 0, -0.6], relativeTo: headAnchor)
}
Move entities relative to device transform
This sample contains a hummingbird that reacts to the wearer while they move around. It achieves this by creating a System and using queryDeviceAnchor to update the entities in the scene with each scene update.
You can only use queryDeviceAnchor in an immersive space, but it doesn’t require authorization.
Note
queryDeviceAnchor gives you the transform of the device, not the wearer’s head. If you want to get the visual transform of the center of the wearer’s head, use AnchorEntity(.head).
The sample starts by creating a RealityKit system, which allows you to update the entities with each scene update. See Implementing systems for entities in a scene for information on creating a system class and using components to query entities. In the system, the app creates a query for entities with the FollowComponent WorldTrackingProvider and an ARKitSession as follows:
public struct FollowSystem: System {
    static let query = EntityQuery(where: .has(FollowComponent.self))
    private let arkitSession = ARKitSession()
    private let worldTrackingProvider = WorldTrackingProvider()
    //...
}
Then, the sample starts the session by using the ARKitSession to run the WorldTrackingProvder.
public struct FollowSystem: System {
    //...


    public init(scene: RealityKit.Scene) {
        startSession()
    }


    func startSession() {
        Task {
            do {
                try await arkitSession.run([worldTrackingProvider])
            } catch {
                print("Error: \(error)")
            }
        }
    }


    //...
}
The sample adds a custom Component named FollowComponent to the root entity of the hummingbird entity, and then uses it to query the entities in the scene to apply the movement to.
Important
Make sure to register both the system and the component.
The following example shows how to query the device anchor and move the entity accordingly:
public struct FollowSystem: System {
    //...


    public func update(context: SceneUpdateContext) {
        // Check whether the world-tracking provider is running.
        guard worldTrackingProvider.state == .running else { return }
        
        // Query the device anchor at the current time.
        guard let deviceAnchor = worldTrackingProvider.queryDeviceAnchor(atTimestamp: CACurrentMediaTime()) else { return }
        
        // Find the transform of the device.
        let deviceTransform = Transform(matrix: deviceAnchor.originFromAnchorTransform)
        
        // Iterate through each entity in the scene containing `FollowComponent`.
        let entities = context.entities(matching: Self.query, updatingSystemWhen: .rendering)
        
        for entity in entities {
            // Move the entity to the device's transform.
            entity.move(to: deviceTransform, relativeTo: entity.parent, duration: 1.2, timingFunction: .easeInOut)
        }
    }
}
The sample keeps the hummingbird at the top right of the wearer’s field of vision by setting the hummingbird’s position relative to its root entity and offsetting it on the y and z axes.
func startFollowMode() {
    //...
    
    // Set the hummingbird as a subentity of its root, and move it to the top-right corner.
    followRoot.addChild(hummingbird)
    hummingbird.setPosition([0.4, 0.2, -1], relativeTo: followRoot)
}
